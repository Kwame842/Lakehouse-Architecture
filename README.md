# Lakehouse Pipeline for E-Commerce (AWS)

This project implements a **serverless, production-grade Lakehouse architecture** on AWS for ingesting, transforming, and analyzing e-commerce transactional data.

It covers the full end-to-end workflow, including data ingestion from Excel and CSV files, data deduplication with Delta Lake, metadata cataloging in Glue, orchestration with Step Functions, and querying via Athena.

---

## Architecture Overview

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚      GitHub Actions (CI/CD)    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚      AWS EventBridge           â”‚
                â”‚ Triggers on new S3 uploads     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     AWS Step Functions         â”‚
                â”‚ ETL Orchestration + Branching  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw S3     â”‚â”€â”€â–¶ â”‚ AWS Lambda (Excelâ†’CSV)â”‚ â”€â”€â–¶â”‚ Glue Jobs (Spark)  â”‚
â”‚ uploads/     â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚                              â”‚
                            â–¼                              â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ S3 /lakehouse/raw/ â”‚        â”‚ S3 /lakehouse/processed/   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                                       â–¼
                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                          â”‚ Glue Data Catalog + Athena â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Features

- âœ… **Trigger-based** data ingestion using EventBridge
- âœ… Converts Excel files into per-sheet CSVs with AWS Lambda
- âœ… Distributed ETL using AWS Glue (PySpark + Delta Lake)
- âœ… ACID-compliant Delta Lake tables stored in S3
- âœ… Metadata registration in Glue Data Catalog
- âœ… Query-ready via Amazon Athena
- âœ… Resumable & fault-tolerant workflows with Step Functions
- âœ… GitHub Actions for CI/CD deployment

---

## Project Structure

```
lakehouse-pipeline/
â”œâ”€â”€ glue_jobs/                 # Spark + Python Shell Glue Jobs
â”‚   â”œâ”€â”€ etl_products.py
â”‚   â”œâ”€â”€ etl_orders.py
â”‚   â”œâ”€â”€ etl_order_items.py
â”‚   â”œâ”€â”€ archive_raw_csvs.py
â”‚   â””â”€â”€ convert_excel_to_csv.py
â”‚
â”œâ”€â”€ step_functions/
â”‚   â””â”€â”€ lakehouse_etl_flow.json     # Resumable Step Function definition
â”‚
â”œâ”€â”€ sql/                       # Athena query samples
â”‚   â”œâ”€â”€ validate_orders.sql
â”‚   â”œâ”€â”€ top_customers.sql
â”‚   â””â”€â”€ rejected_records_analysis.sql
â”‚
â”œâ”€â”€ scripts/                   # Optional data CLI/utilities (optional)
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ deploy.yml         # CI/CD Pipeline
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ETL Workflow Logic (Orchestration)

### Trigger

- Triggered by new `.csv` or `.xlsx` files uploaded to `s3://ecom-lakehouse/uploads/`

### Steps

1. **Lambda**: Converts `.xlsx` â†’ `.csv` per sheet
2. **Marker Check**: `check_marker_lambda` avoids reprocessing files
3. **Glue Jobs**:
   - `etl_products`: Deduplicates and stores product info
   - `etl_orders`: Cleans, timestamps, partitions order data
   - `etl_order_items`: Ensures FK integrity (joins to product)
4. **Delta Writes**: Data stored in `/lakehouse/processed/` in Delta format
5. **Glue Table Registration**: Athena-ready with proper schema
6. **Archival**: Raw CSVs moved to `/archived/` once successfully processed

### Failure Handling

- Branching logic allows job retries or skips gracefully
- System resumes from last unprocessed dataset

---

## Validation Rules

| Rule                          | Applied To                      |
| ----------------------------- | ------------------------------- |
| No null `product_id`          | `etl_products`                  |
| Deduplicate by primary keys   | All jobs                        |
| Valid `order_timestamp`       | `etl_orders`, `etl_order_items` |
| FK integrity (`product_id`)   | `etl_order_items`               |
| Partition by `date`           | `etl_orders`, `etl_order_items` |
| Log rejected records (future) | Extendable                      |

---

## Athena Queries

Use the Glue Data Catalog database: `ecom_catalog`

```sql
-- Preview the first 10 rows for the Orders table
SELECT * 
FROM "ecom_catalog"."orders" 
limit 10;
```

More queries: `sql/`

![Athena Query](imgs/Athena.png)

---

## CI/CD (GitHub Actions)

### `.github/workflows/deploy.yml` handles:

- âœ… Dependency installs
- âœ… Pytest validation (optional)
- âœ… Upload Glue scripts to S3
- âœ… Deploy Step Function via CLI

```bash
# Triggered on push to main
git push origin main
```

---

## Local Dev Requirements

```bash
# requirements.txt
boto3
pandas
openpyxl
pytest
```

You can run scripts locally if needed:

```bash
python glue_jobs/convert_excel_to_csv.py
```

---

## Deploy

### 1. Upload Glue Jobs

```bash
aws s3 cp glue_jobs/ s3://ecom-lakehouse/scripts/ --recursive
```

### 2. Deploy Step Function

```bash
aws stepfunctions update-state-machine   --state-machine-arn arn:aws:states:<region>:<acct>:stateMachine:lakehouse-orchestration   --definition file://step_functions/lakehouse_etl_flow.json
```

![Step Functions](imgs/StepFunction.png)

---

## Assumptions

- Raw files are always placed under `s3://ecom-lakehouse/uploads/`
- All `.xlsx` are multi-sheet; converted to one CSV per sheet
- Each sheet is uniquely named (avoid collisions)
- Glue job marker check is backed by S3 object or DynamoDB key (customizable)

---

## Future Extensions

- â— Rejected record logging to `/rejected/`
- ğŸ“© Add SNS/email alerts for ETL failure
- ğŸ§¹ Partition compaction via OPTIMIZE (Delta)
- ğŸ§ª Unit tests for each job in `tests/` folder

---

## Contributors

- **Architect & Engineer**: Kwame A. Boateng
- Contact: *kayboateng@email.com*

---

## License

MIT License 
